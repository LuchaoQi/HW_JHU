{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 492,
     "status": "ok",
     "timestamp": 1555200365966,
     "user": {
      "displayName": "Xi Wang",
      "photoUrl": "",
      "userId": "14282931480726339478"
     },
     "user_tz": 240
    },
    "id": "8JnxRZdFW5Ao",
    "outputId": "94987853-3e51-40bb-82e2-0e971afa362d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/mydrive; to attempt to forcibly remount, call drive.mount(\"/content/mydrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "## If dataset folder is the same directory as the script in Google Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/mydrive')\n",
    "# path = '/content/mydrive/My Drive/Colab Notebooks/DL/HW6/Hw6_Q1_dataset/HW6_data/'\n",
    "\n",
    "## Local Jupyter Notebook\n",
    "path = '/Hw6_Q1_dataset/HW6_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eVm_vt6fxn9V"
   },
   "outputs": [],
   "source": [
    "# This code is provided for Deep Learning class (CS 482/682) Homework 6 practice.\n",
    "# This is a sketch code for main function. There are some given hyper-parameters insideself.\n",
    "# You need to finish the design and train your network.\n",
    "# @Copyright Cong Gao, the Johns Hopkins University, cgao11@jhu.edu\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "######################## Hyperparameters #################################\n",
    "# Batch size can be changed if it does not match your memory, please state your batch step_size\n",
    "# in your report.\n",
    "train_batch_size = 30\n",
    "validation_batch_size=10\n",
    "# Please use this learning rate for Q(a) and Q(b)\n",
    "learning_rate = 0.005 ## Originally 0.001\n",
    "# This num_epochs is designed for running to be long enough, you need to manually stop or design\n",
    "# your early stopping method.\n",
    "num_epochs = 1000\n",
    "\n",
    "# Design your own dataloader\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, input_dir):\n",
    "        self.path = input_dir\n",
    "\n",
    "    def __len__ (self):\n",
    "        folder_name = self.path.split('/')[-2]\n",
    "        if folder_name == 'train_cor':\n",
    "            return 1584\n",
    "        else:\n",
    "            return 50\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        img_temp = str(idx) + \"/\" + str(idx) + \"_gray.jpg\"\n",
    "        img_path = os.path.join(self.path, img_temp)\n",
    "        mask_temp = str(idx) + \"/\" + str(idx) + \"_input.jpg\"\n",
    "        mask_path = os.path.join(self.path, mask_temp)\n",
    "        \n",
    "        img = plt.imread(img_path)\n",
    "        img = np.atleast_3d(img).transpose(2,0,1).astype(np.float32)\n",
    "        img = torch.from_numpy(img).float()\n",
    "        \n",
    "        mask = plt.imread(mask_path)\n",
    "#         mask = (mask * 255).round().astype(np.uint8)\n",
    "        mask = np.atleast_3d(mask).transpose(2,0,1).astype(np.float32)\n",
    "        mask = torch.from_numpy(mask).float()\n",
    "#         oHot = mask == 0\n",
    "#         for i in range(1,8):\n",
    "#             oHot = torch.cat([oHot, mask == i*32])\n",
    "            \n",
    "        return img, mask\n",
    "\n",
    "train_dataset=ImageDataset(input_dir = path+'colorization/train_cor/')\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=train_batch_size,\n",
    "                                           shuffle=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VxI3NAsuWzyk",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This code is provided for Deep Learning class (CS 482/682) Homework 6 practice.\n",
    "# The network structure is a simplified U-net. You need to finish the last layers\n",
    "# @Copyright Cong Gao, the Johns Hopkins University, cgao11@jhu.edu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "import numpy as np\n",
    "\n",
    "def add_conv_stage(dim_in, dim_out, kernel_size=3, stride=1, padding=1, bias=True, useBN=True):\n",
    "  if useBN:\n",
    "    return nn.Sequential(\n",
    "      nn.Conv2d(dim_in, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "      nn.BatchNorm2d(dim_out),\n",
    "      nn.LeakyReLU(0.1),\n",
    "      nn.Conv2d(dim_out, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "      nn.BatchNorm2d(dim_out),\n",
    "      nn.LeakyReLU(0.1)\n",
    "    )\n",
    "  else:\n",
    "    return nn.Sequential(\n",
    "      nn.Conv2d(dim_in, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(dim_out, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "      nn.ReLU()\n",
    "    )\n",
    "\n",
    "def add_merge_stage(ch_coarse, ch_fine, in_coarse, in_fine, upsample):\n",
    "  conv = nn.ConvTranspose2d(ch_coarse, ch_fine, 4, 2, 1, bias=False)\n",
    "  torch.cat(conv, in_fine)\n",
    "\n",
    "  return nn.Sequential(\n",
    "    nn.ConvTranspose2d(ch_coarse, ch_fine, 4, 2, 1, bias=False)\n",
    "  )\n",
    "  upsample(in_coarse)\n",
    "\n",
    "def upsample(ch_coarse, ch_fine):\n",
    "  return nn.Sequential(\n",
    "    nn.ConvTranspose2d(ch_coarse, ch_fine, 4, 2, 1, bias=False),\n",
    "    nn.ReLU()\n",
    "  )\n",
    "\n",
    "class unet(nn.Module):\n",
    "  def __init__(self, useBN=True):\n",
    "    super(unet, self).__init__()\n",
    "    # Downgrade stages\n",
    "    self.conv1   = add_conv_stage(1, 32, useBN=useBN)\n",
    "    self.conv1E   = add_conv_stage(3, 32, useBN=useBN)\n",
    "    self.conv2   = add_conv_stage(32, 64, useBN=useBN)\n",
    "    self.conv3   = add_conv_stage(64, 128, useBN=useBN)\n",
    "    self.conv4   = add_conv_stage(128, 256, useBN=useBN)\n",
    "    # Upgrade stages\n",
    "    self.conv4m = add_conv_stage(512, 256, useBN=useBN)\n",
    "    self.conv3m = add_conv_stage(256, 128, useBN=useBN)\n",
    "    self.conv2m = add_conv_stage(128,  64, useBN=useBN)\n",
    "    self.conv1m = add_conv_stage( 64,  32, useBN=useBN)\n",
    "    # Maxpool\n",
    "    self.max_pool = nn.MaxPool2d(2)\n",
    "    # Upsample layers\n",
    "    self.upsample54 = upsample(512, 256)\n",
    "    self.upsample43 = upsample(256, 128)\n",
    "    self.upsample32 = upsample(128,  64)\n",
    "    self.upsample21 = upsample(64 ,  32)\n",
    "    ## weight initialization\n",
    "    for m in self.modules():\n",
    "      if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "          m.bias.data.zero_()\n",
    "\n",
    "    ## Design your last layer & activations\n",
    "    self.conv_last = nn.Conv2d(32, 3, 1)\n",
    "    self.conv_lastE = nn.Conv2d(32, 8, 1)\n",
    "    self.activation = nn.LeakyReLU()\n",
    "    self.activationE = nn.Sigmoid()\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    conv1_out = self.conv1(x)\n",
    "    conv2_out = self.conv2(self.max_pool(conv1_out))\n",
    "    conv3_out = self.conv3(self.max_pool(conv2_out))\n",
    "    conv4_out = self.conv4(self.max_pool(conv3_out))\n",
    "\n",
    "    conv4m_out_ = torch.cat((self.upsample43(conv4_out), conv3_out), 1)\n",
    "    conv3m_out = self.conv3m(conv4m_out_)\n",
    "\n",
    "    conv3m_out_ = torch.cat((self.upsample32(conv3m_out), conv2_out), 1)\n",
    "    conv2m_out = self.conv2m(conv3m_out_)\n",
    "\n",
    "    conv2m_out_ = torch.cat((self.upsample21(conv2m_out), conv1_out), 1)\n",
    "    conv1m_out = self.conv1m(conv2m_out_)\n",
    "\n",
    "    ## Design your last layer & activations\n",
    "    out = self.conv_last(conv1m_out)\n",
    "    out = self.activation(out)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3038
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3146993,
     "status": "error",
     "timestamp": 1555203512498,
     "user": {
      "displayName": "Xi Wang",
      "photoUrl": "",
      "userId": "14282931480726339478"
     },
     "user_tz": 240
    },
    "id": "ohneYiithP69",
    "outputId": "0f923fc8-c608-4583-e04a-0c7200529056"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started!\n",
      "\n",
      "EPOCH 1 of 1000\n",
      "\n",
      ". . . . . Training Loss: 11145.8128\n",
      "2m 11s\n",
      "\n",
      "EPOCH 2 of 1000\n",
      "\n",
      ". . . . . Training Loss: 9043.9377\n",
      "2m 11s\n",
      "\n",
      "EPOCH 3 of 1000\n",
      "\n",
      ". . . . . Training Loss: 5457.3239\n",
      "2m 11s\n",
      "\n",
      "EPOCH 4 of 1000\n",
      "\n",
      ". . . . . Training Loss: 2349.0030\n",
      "2m 11s\n",
      "\n",
      "EPOCH 5 of 1000\n",
      "\n",
      ". . . . . Training Loss: 876.2239\n",
      "2m 10s\n",
      "\n",
      "EPOCH 6 of 1000\n",
      "\n",
      ". . . . . Training Loss: 350.2613\n",
      "2m 10s\n",
      "\n",
      "EPOCH 7 of 1000\n",
      "\n",
      ". . . . . Training Loss: 177.0275\n",
      "2m 10s\n",
      "\n",
      "EPOCH 8 of 1000\n",
      "\n",
      ". . . . . Training Loss: 114.5584\n",
      "2m 10s\n",
      "\n",
      "EPOCH 9 of 1000\n",
      "\n",
      ". . . . . Training Loss: 83.1981\n",
      "2m 10s\n",
      "\n",
      "EPOCH 10 of 1000\n",
      "\n",
      ". . . . . Training Loss: 66.6561\n",
      "2m 10s\n",
      "\n",
      "EPOCH 11 of 1000\n",
      "\n",
      ". . . . . Training Loss: 59.3963\n",
      "2m 10s\n",
      "\n",
      "EPOCH 12 of 1000\n",
      "\n",
      ". . . . . Training Loss: 49.3784\n",
      "2m 10s\n",
      "\n",
      "EPOCH 13 of 1000\n",
      "\n",
      ". . . . . Training Loss: 43.7063\n",
      "2m 10s\n",
      "\n",
      "EPOCH 14 of 1000\n",
      "\n",
      ". . . . . Training Loss: 42.6449\n",
      "2m 10s\n",
      "\n",
      "EPOCH 15 of 1000\n",
      "\n",
      ". . . . . Training Loss: 38.3317\n",
      "2m 10s\n",
      "\n",
      "EPOCH 16 of 1000\n",
      "\n",
      ". . . . . Training Loss: 36.2886\n",
      "2m 11s\n",
      "\n",
      "EPOCH 17 of 1000\n",
      "\n",
      ". . . . . Training Loss: 33.2858\n",
      "2m 11s\n",
      "\n",
      "EPOCH 18 of 1000\n",
      "\n",
      ". . . . . Training Loss: 31.9879\n",
      "2m 12s\n",
      "\n",
      "EPOCH 19 of 1000\n",
      "\n",
      ". . . . . Training Loss: 28.9499\n",
      "2m 11s\n",
      "\n",
      "EPOCH 20 of 1000\n",
      "\n",
      ". . . . . Training Loss: 27.7216\n",
      "2m 10s\n",
      "\n",
      "EPOCH 21 of 1000\n",
      "\n",
      ". . . . . Training Loss: 27.0643\n",
      "2m 10s\n",
      "\n",
      "EPOCH 22 of 1000\n",
      "\n",
      ". . . . . Training Loss: 25.0145\n",
      "2m 10s\n",
      "\n",
      "EPOCH 23 of 1000\n",
      "\n",
      ". . . . . Training Loss: 23.6395\n",
      "2m 10s\n",
      "\n",
      "EPOCH 24 of 1000\n",
      "\n",
      ". . . . . Training Loss: 22.2400\n",
      "2m 10s\n",
      "\n",
      "EPOCH 25 of 1000\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a89dc8121b33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-3e8db100651b>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;31m#         mask = (mask * 255).round().astype(np.uint8)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   2150\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_dedent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2152\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1349\u001b[0m                              \u001b[0;34m'with Pillow installed matplotlib can handle '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m                              'more images' % list(handlers))\n\u001b[0;32m-> 1351\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1352\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpil_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2530\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2531\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = unet();\n",
    "\n",
    "if torch.cuda.is_available(): #use gpu if available\n",
    "    model.cuda()\n",
    "\n",
    "import time\n",
    "import copy\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(\"Training Started!\")\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "converge_epoch = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    ########################### Training #####################################\n",
    "    print(\"\\nEPOCH \" +str(epoch+1)+\" of \"+str(num_epochs)+\"\\n\")\n",
    "    # Please design your own training section\n",
    "    since = time.time()\n",
    "    losses=[]\n",
    "    bar = 0\n",
    "    for _, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.float())\n",
    "        labels = Variable(labels.float())\n",
    "        \n",
    "        # Regular Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images.cuda())\n",
    "        loss = criterion(outputs, labels.cuda())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.data.item())\n",
    "        bar += 1\n",
    "        if bar == 10:\n",
    "            print('.', end=\" \")\n",
    "            bar = 0\n",
    "    ### Average Batch Loss\n",
    "    mean_loss = sum(losses)/len(losses)\n",
    "#     if mean_loss > 100000:\n",
    "#         mean_loss = train_losses[epoch-1]\n",
    "    print(\"Training Loss: %.4f\" % (mean_loss))\n",
    "\n",
    "    train_losses.append(mean_loss)\n",
    "    converge_epoch.append(epoch)\n",
    "    \n",
    "    ### Timing\n",
    "    time_elapsed = time.time() - since\n",
    "    print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "#     ### Early Stopping\n",
    "#     if epoch >= 300 and abs(mean_loss - val_losses[epoch-1]) <= 1:\n",
    "#         print('The validation loss converges')\n",
    "#         break\n",
    "plt.plot(converge_epoch, train_losses, label = \"training loss\")\n",
    "plt.xlabel('epoches')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 427,
     "status": "ok",
     "timestamp": 1555203515050,
     "user": {
      "displayName": "Xi Wang",
      "photoUrl": "",
      "userId": "14282931480726339478"
     },
     "user_tz": 240
    },
    "id": "gfRS-jMeO7rK",
    "outputId": "48c750f9-8a95-438f-9df3-9846279f1724"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEKCAYAAADaa8itAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(converge_epoch, train_losses, label = \"training loss\")\n",
    "# plt.plot(converge_epoch, val_losses, label = \"validation loss\")\n",
    "plt.xlabel('epoches')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yusfwz_ib5DF"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), path+'yourmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GgqA56p3vBoT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Xi_Wang_HW6_Q1d.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
