{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 772,
     "status": "ok",
     "timestamp": 1555124600807,
     "user": {
      "displayName": "Xi Wang",
      "photoUrl": "",
      "userId": "14282931480726339478"
     },
     "user_tz": 240
    },
    "id": "8JnxRZdFW5Ao",
    "outputId": "a33bdeac-e7b6-42ad-c033-9757169e6a1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/mydrive; to attempt to forcibly remount, call drive.mount(\"/content/mydrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "## If dataset folder is the same directory as the script in Google Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/mydrive')\n",
    "# path = '/content/mydrive/My Drive/Colab Notebooks/DL/HW6/Hw6_Q1_dataset/HW6_data/'\n",
    "\n",
    "## Local Jupyter Notebook\n",
    "path = '/Hw6_Q1_dataset/HW6_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eVm_vt6fxn9V"
   },
   "outputs": [],
   "source": [
    "# This code is provided for Deep Learning class (CS 482/682) Homework 6 practice.\n",
    "# This is a sketch code for main function. There are some given hyper-parameters insideself.\n",
    "# You need to finish the design and train your network.\n",
    "# @Copyright Cong Gao, the Johns Hopkins University, cgao11@jhu.edu\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "######################## Hyperparameters #################################\n",
    "# Batch size can be changed if it does not match your memory, please state your batch step_size\n",
    "# in your report.\n",
    "train_batch_size = 10\n",
    "validation_batch_size=10\n",
    "# Please use this learning rate for Q(a) and Q(b)\n",
    "learning_rate = 0.002 ## Originally 0.001\n",
    "# This num_epochs is designed for running to be long enough, you need to manually stop or design\n",
    "# your early stopping method.\n",
    "num_epochs = 1000\n",
    "\n",
    "# Design your own dataloader\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, input_dir):\n",
    "        self.path = input_dir\n",
    "\n",
    "    def __len__ (self):\n",
    "        folder_name = self.path.split('/')[-1]\n",
    "        if folder_name == 'train':\n",
    "            return 300\n",
    "        else:\n",
    "            return 50\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        img_temp = str(idx) + \"/\" + str(idx) + \"_input.jpg\"\n",
    "        img_path = os.path.join(self.path, img_temp)\n",
    "        mask_temp = str(idx) + \"/\" + str(idx) + \"_mask.png\"\n",
    "        mask_path = os.path.join(self.path, mask_temp)\n",
    "        \n",
    "        img = plt.imread(img_path)\n",
    "        img = np.atleast_3d(img).transpose(2,0,1).astype(np.float32)\n",
    "        img = torch.from_numpy(img).float()\n",
    "        \n",
    "        mask = plt.imread(mask_path)\n",
    "        mask = (mask * 255).round().astype(np.uint8)\n",
    "        mask = np.atleast_3d(mask).transpose(2,0,1).astype(np.float32)\n",
    "        mask = torch.from_numpy(mask).float()\n",
    "        oHot = mask == 0\n",
    "        for i in range(1,8):\n",
    "            oHot = torch.cat([oHot, mask == i*32])\n",
    "            \n",
    "        return img, oHot\n",
    "\n",
    "train_dataset=ImageDataset(input_dir = path+'segmentation/train/')\n",
    "validation_dataset=ImageDataset(input_dir = path+'segmentation/validation/' )\n",
    "test_dataset=ImageDataset(input_dir = path+'segmentation/test/' )\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=train_batch_size,\n",
    "                                           shuffle=True);\n",
    "validation_loader = DataLoader(dataset=validation_dataset,\n",
    "                                          batch_size=validation_batch_size,\n",
    "                                          shuffle=True);\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=validation_batch_size,\n",
    "                                          shuffle=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VxI3NAsuWzyk",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This code is provided for Deep Learning class (CS 482/682) Homework 6 practice.\n",
    "# The network structure is a simplified U-net. You need to finish the last layers\n",
    "# @Copyright Cong Gao, the Johns Hopkins University, cgao11@jhu.edu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "import numpy as np\n",
    "\n",
    "def add_conv_stage(dim_in, dim_out, kernel_size=3, stride=1, padding=1, bias=True, useBN=True):\n",
    "  if useBN:\n",
    "    return nn.Sequential(\n",
    "      nn.Conv2d(dim_in, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "      nn.BatchNorm2d(dim_out),\n",
    "      nn.LeakyReLU(0.1),\n",
    "      nn.Conv2d(dim_out, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "      nn.BatchNorm2d(dim_out),\n",
    "      nn.LeakyReLU(0.1)\n",
    "    )\n",
    "  else:\n",
    "    return nn.Sequential(\n",
    "      nn.Conv2d(dim_in, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(dim_out, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "      nn.ReLU()\n",
    "    )\n",
    "\n",
    "def add_merge_stage(ch_coarse, ch_fine, in_coarse, in_fine, upsample):\n",
    "  conv = nn.ConvTranspose2d(ch_coarse, ch_fine, 4, 2, 1, bias=False)\n",
    "  torch.cat(conv, in_fine)\n",
    "\n",
    "  return nn.Sequential(\n",
    "    nn.ConvTranspose2d(ch_coarse, ch_fine, 4, 2, 1, bias=False)\n",
    "  )\n",
    "  upsample(in_coarse)\n",
    "\n",
    "def upsample(ch_coarse, ch_fine):\n",
    "  return nn.Sequential(\n",
    "    nn.ConvTranspose2d(ch_coarse, ch_fine, 4, 2, 1, bias=False),\n",
    "    nn.ReLU()\n",
    "  )\n",
    "\n",
    "class unet(nn.Module):\n",
    "  def __init__(self, useBN=True):\n",
    "    super(unet, self).__init__()\n",
    "    # Downgrade stages\n",
    "    self.conv1   = add_conv_stage(3, 32, useBN=useBN)\n",
    "    self.conv2   = add_conv_stage(32, 64, useBN=useBN)\n",
    "    self.conv3   = add_conv_stage(64, 128, useBN=useBN)\n",
    "    self.conv4   = add_conv_stage(128, 256, useBN=useBN)\n",
    "    # Upgrade stages\n",
    "    self.conv4m = add_conv_stage(512, 256, useBN=useBN)\n",
    "    self.conv3m = add_conv_stage(256, 128, useBN=useBN)\n",
    "    self.conv2m = add_conv_stage(128,  64, useBN=useBN)\n",
    "    self.conv1m = add_conv_stage( 64,  32, useBN=useBN)\n",
    "    # Maxpool\n",
    "    self.max_pool = nn.MaxPool2d(2)\n",
    "    # Upsample layers\n",
    "    self.upsample54 = upsample(512, 256)\n",
    "    self.upsample43 = upsample(256, 128)\n",
    "    self.upsample32 = upsample(128,  64)\n",
    "    self.upsample21 = upsample(64 ,  32)\n",
    "    ## weight initialization\n",
    "    for m in self.modules():\n",
    "      if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "          m.bias.data.zero_()\n",
    "\n",
    "    ## Design your last layer & activations\n",
    "    self.conv_last = nn.Conv2d(32, 8, 1)\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    conv1_out = self.conv1(x)\n",
    "    conv2_out = self.conv2(self.max_pool(conv1_out))\n",
    "    conv3_out = self.conv3(self.max_pool(conv2_out))\n",
    "    conv4_out = self.conv4(self.max_pool(conv3_out))\n",
    "\n",
    "    conv4m_out_ = torch.cat((self.upsample43(conv4_out), conv3_out), 1)\n",
    "    conv3m_out = self.conv3m(conv4m_out_)\n",
    "\n",
    "    conv3m_out_ = torch.cat((self.upsample32(conv3m_out), conv2_out), 1)\n",
    "    conv2m_out = self.conv2m(conv3m_out_)\n",
    "\n",
    "    conv2m_out_ = torch.cat((self.upsample21(conv2m_out), conv1_out), 1)\n",
    "    conv1m_out = self.conv1m(conv2m_out_)\n",
    "\n",
    "    ## Design your last layer & activations\n",
    "    out = self.conv_last(conv1m_out)\n",
    "    out = torch.sigmoid(out)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8OrYRo_Ew0d2"
   },
   "outputs": [],
   "source": [
    "def dice_loss(output, labels):\n",
    "  output = output.contiguous()\n",
    "  labels = labels.contiguous()\n",
    "  \n",
    "  tp = (output*labels).sum(dim=2).sum(dim=2)\n",
    "  tpfp = output.sum(dim=2).sum(dim=2)\n",
    "  tpfn = labels.sum(dim=2).sum(dim=2)\n",
    "  loss = (1 - ( (2.*tp + 1e-7) / (tpfp+tpfn+1e-7) ) )\n",
    "  \n",
    "  return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32455
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1630812,
     "status": "error",
     "timestamp": 1555126230896,
     "user": {
      "displayName": "Xi Wang",
      "photoUrl": "",
      "userId": "14282931480726339478"
     },
     "user_tz": 240
    },
    "id": "ohneYiithP69",
    "outputId": "41dc9cae-0155-4fb0-9eaf-02168ad27982"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started!\n",
      "\n",
      "EPOCH 1 of 1000\n",
      "\n",
      "Training Loss: 0.8835\n",
      "0m 4s\n",
      "Validation Loss: 0.8843\n",
      "0m 3s\n",
      "\n",
      "EPOCH 2 of 1000\n",
      "\n",
      "Training Loss: 0.8642\n",
      "0m 3s\n",
      "Validation Loss: 0.8612\n",
      "0m 3s\n",
      "\n",
      "EPOCH 3 of 1000\n",
      "\n",
      "Training Loss: 0.8363\n",
      "0m 3s\n",
      "Validation Loss: 0.8597\n",
      "0m 3s\n",
      "\n",
      "EPOCH 4 of 1000\n",
      "\n",
      "Training Loss: 0.8353\n",
      "0m 3s\n",
      "Validation Loss: 0.8556\n",
      "0m 3s\n",
      "\n",
      "EPOCH 5 of 1000\n",
      "\n",
      "Training Loss: 0.8308\n",
      "0m 3s\n",
      "Validation Loss: 0.8490\n",
      "0m 3s\n",
      "\n",
      "EPOCH 6 of 1000\n",
      "\n",
      "Training Loss: 0.8058\n",
      "0m 3s\n",
      "Validation Loss: 0.8435\n",
      "0m 3s\n",
      "\n",
      "EPOCH 7 of 1000\n",
      "\n",
      "Training Loss: 0.7952\n",
      "0m 3s\n",
      "Validation Loss: 0.8213\n",
      "0m 3s\n",
      "\n",
      "EPOCH 8 of 1000\n",
      "\n",
      "Training Loss: 0.7636\n",
      "0m 3s\n",
      "Validation Loss: 0.8236\n",
      "0m 3s\n",
      "\n",
      "EPOCH 9 of 1000\n",
      "\n",
      "Training Loss: 0.7659\n",
      "0m 3s\n",
      "Validation Loss: 0.8178\n",
      "0m 3s\n",
      "\n",
      "EPOCH 10 of 1000\n",
      "\n",
      "Training Loss: 0.7688\n",
      "0m 3s\n",
      "Validation Loss: 0.8265\n",
      "0m 3s\n",
      "\n",
      "EPOCH 11 of 1000\n",
      "\n",
      "Training Loss: 0.7585\n",
      "0m 3s\n",
      "Validation Loss: 0.8152\n",
      "0m 3s\n",
      "\n",
      "EPOCH 12 of 1000\n",
      "\n",
      "Training Loss: 0.7639\n",
      "0m 3s\n",
      "Validation Loss: 0.8224\n",
      "0m 3s\n",
      "\n",
      "EPOCH 13 of 1000\n",
      "\n",
      "Training Loss: 0.7601\n",
      "0m 3s\n",
      "Validation Loss: 0.8155\n",
      "0m 3s\n",
      "\n",
      "EPOCH 14 of 1000\n",
      "\n",
      "Training Loss: 0.7515\n",
      "0m 3s\n",
      "Validation Loss: 0.8131\n",
      "0m 3s\n",
      "\n",
      "EPOCH 15 of 1000\n",
      "\n",
      "Training Loss: 0.7533\n",
      "0m 3s\n",
      "Validation Loss: 0.8105\n",
      "0m 3s\n",
      "\n",
      "EPOCH 16 of 1000\n",
      "\n",
      "Training Loss: 0.7617\n",
      "0m 3s\n",
      "Validation Loss: 0.8181\n",
      "0m 3s\n",
      "\n",
      "EPOCH 17 of 1000\n",
      "\n",
      "Training Loss: 0.7388\n",
      "0m 3s\n",
      "Validation Loss: 0.8149\n",
      "0m 3s\n",
      "\n",
      "EPOCH 18 of 1000\n",
      "\n",
      "Training Loss: 0.7487\n",
      "0m 3s\n",
      "Validation Loss: 0.8116\n",
      "0m 3s\n",
      "\n",
      "EPOCH 19 of 1000\n",
      "\n",
      "Training Loss: 0.7420\n",
      "0m 3s\n",
      "Validation Loss: 0.8105\n",
      "0m 3s\n",
      "\n",
      "EPOCH 20 of 1000\n",
      "\n",
      "Training Loss: 0.7187\n",
      "0m 3s\n",
      "Validation Loss: 0.8083\n",
      "0m 3s\n",
      "\n",
      "EPOCH 21 of 1000\n",
      "\n",
      "Training Loss: 0.7293\n",
      "0m 3s\n",
      "Validation Loss: 0.8091\n",
      "0m 3s\n",
      "\n",
      "EPOCH 22 of 1000\n",
      "\n",
      "Training Loss: 0.7342\n",
      "0m 3s\n",
      "Validation Loss: 0.8088\n",
      "0m 3s\n",
      "\n",
      "EPOCH 23 of 1000\n",
      "\n",
      "Training Loss: 0.7322\n",
      "0m 3s\n",
      "Validation Loss: 0.8040\n",
      "0m 3s\n",
      "\n",
      "EPOCH 24 of 1000\n",
      "\n",
      "Training Loss: 0.7204\n",
      "0m 3s\n",
      "Validation Loss: 0.8049\n",
      "0m 3s\n",
      "\n",
      "EPOCH 25 of 1000\n",
      "\n",
      "Training Loss: 0.7162\n",
      "0m 3s\n",
      "Validation Loss: 0.8029\n",
      "0m 3s\n",
      "\n",
      "EPOCH 26 of 1000\n",
      "\n",
      "Training Loss: 0.7096\n",
      "0m 3s\n",
      "Validation Loss: 0.8055\n",
      "0m 3s\n",
      "\n",
      "EPOCH 27 of 1000\n",
      "\n",
      "Training Loss: 0.7038\n",
      "0m 3s\n",
      "Validation Loss: 0.8138\n",
      "0m 3s\n",
      "\n",
      "EPOCH 28 of 1000\n",
      "\n",
      "Training Loss: 0.7335\n",
      "0m 3s\n",
      "Validation Loss: 0.8149\n",
      "0m 3s\n",
      "\n",
      "EPOCH 29 of 1000\n",
      "\n",
      "Training Loss: 0.7096\n",
      "0m 3s\n",
      "Validation Loss: 0.8058\n",
      "0m 3s\n",
      "\n",
      "EPOCH 30 of 1000\n",
      "\n",
      "Training Loss: 0.7042\n",
      "0m 3s\n",
      "Validation Loss: 0.8056\n",
      "0m 3s\n",
      "\n",
      "EPOCH 31 of 1000\n",
      "\n",
      "Training Loss: 0.7535\n",
      "0m 3s\n",
      "Validation Loss: 0.8512\n",
      "0m 3s\n",
      "\n",
      "EPOCH 32 of 1000\n",
      "\n",
      "Training Loss: 0.7892\n",
      "0m 3s\n",
      "Validation Loss: 0.8200\n",
      "0m 3s\n",
      "\n",
      "EPOCH 33 of 1000\n",
      "\n",
      "Training Loss: 0.7351\n",
      "0m 3s\n",
      "Validation Loss: 0.8027\n",
      "0m 3s\n",
      "\n",
      "EPOCH 34 of 1000\n",
      "\n",
      "Training Loss: 0.7098\n",
      "0m 3s\n",
      "Validation Loss: 0.8026\n",
      "0m 3s\n",
      "\n",
      "EPOCH 35 of 1000\n",
      "\n",
      "Training Loss: 0.7239\n",
      "0m 3s\n",
      "Validation Loss: 0.8016\n",
      "0m 3s\n",
      "\n",
      "EPOCH 36 of 1000\n",
      "\n",
      "Training Loss: 0.6991\n",
      "0m 3s\n",
      "Validation Loss: 0.8031\n",
      "0m 3s\n",
      "\n",
      "EPOCH 37 of 1000\n",
      "\n",
      "Training Loss: 0.7274\n",
      "0m 3s\n",
      "Validation Loss: 0.8030\n",
      "0m 3s\n",
      "\n",
      "EPOCH 38 of 1000\n",
      "\n",
      "Training Loss: 0.7203\n",
      "0m 3s\n",
      "Validation Loss: 0.8050\n",
      "0m 3s\n",
      "\n",
      "EPOCH 39 of 1000\n",
      "\n",
      "Training Loss: 0.7242\n",
      "0m 3s\n",
      "Validation Loss: 0.8102\n",
      "0m 3s\n",
      "\n",
      "EPOCH 40 of 1000\n",
      "\n",
      "Training Loss: 0.7247\n",
      "0m 3s\n",
      "Validation Loss: 0.8087\n",
      "0m 3s\n",
      "\n",
      "EPOCH 41 of 1000\n",
      "\n",
      "Training Loss: 0.7149\n",
      "0m 3s\n",
      "Validation Loss: 0.8034\n",
      "0m 3s\n",
      "\n",
      "EPOCH 42 of 1000\n",
      "\n",
      "Training Loss: 0.7091\n",
      "0m 3s\n",
      "Validation Loss: 0.8050\n",
      "0m 3s\n",
      "\n",
      "EPOCH 43 of 1000\n",
      "\n",
      "Training Loss: 0.6995\n",
      "0m 3s\n",
      "Validation Loss: 0.8024\n",
      "0m 3s\n",
      "\n",
      "EPOCH 44 of 1000\n",
      "\n",
      "Training Loss: 0.7154\n",
      "0m 3s\n",
      "Validation Loss: 0.8008\n",
      "0m 3s\n",
      "\n",
      "EPOCH 45 of 1000\n",
      "\n",
      "Training Loss: 0.7062\n",
      "0m 3s\n",
      "Validation Loss: 0.8032\n",
      "0m 3s\n",
      "\n",
      "EPOCH 46 of 1000\n",
      "\n",
      "Training Loss: 0.7161\n",
      "0m 3s\n",
      "Validation Loss: 0.8022\n",
      "0m 3s\n",
      "\n",
      "EPOCH 47 of 1000\n",
      "\n",
      "Training Loss: 0.7031\n",
      "0m 3s\n",
      "Validation Loss: 0.7999\n",
      "0m 3s\n",
      "\n",
      "EPOCH 48 of 1000\n",
      "\n",
      "Training Loss: 0.7059\n",
      "0m 3s\n",
      "Validation Loss: 0.8019\n",
      "0m 3s\n",
      "\n",
      "EPOCH 49 of 1000\n",
      "\n",
      "Training Loss: 0.7157\n",
      "0m 3s\n",
      "Validation Loss: 0.7997\n",
      "0m 3s\n",
      "\n",
      "EPOCH 50 of 1000\n",
      "\n",
      "Training Loss: 0.6761\n",
      "0m 3s\n",
      "Validation Loss: 0.7988\n",
      "0m 3s\n",
      "\n",
      "EPOCH 51 of 1000\n",
      "\n",
      "Training Loss: 0.7072\n",
      "0m 3s\n",
      "Validation Loss: 0.8006\n",
      "0m 3s\n",
      "\n",
      "EPOCH 52 of 1000\n",
      "\n",
      "Training Loss: 0.7046\n",
      "0m 3s\n",
      "Validation Loss: 0.8038\n",
      "0m 3s\n",
      "\n",
      "EPOCH 53 of 1000\n",
      "\n",
      "Training Loss: 0.6908\n",
      "0m 3s\n",
      "Validation Loss: 0.8114\n",
      "0m 3s\n",
      "\n",
      "EPOCH 54 of 1000\n",
      "\n",
      "Training Loss: 0.7217\n",
      "0m 3s\n",
      "Validation Loss: 0.8024\n",
      "0m 3s\n",
      "\n",
      "EPOCH 55 of 1000\n",
      "\n",
      "Training Loss: 0.6975\n",
      "0m 3s\n",
      "Validation Loss: 0.8005\n",
      "0m 3s\n",
      "\n",
      "EPOCH 56 of 1000\n",
      "\n",
      "Training Loss: 0.7010\n",
      "0m 3s\n",
      "Validation Loss: 0.8035\n",
      "0m 3s\n",
      "\n",
      "EPOCH 57 of 1000\n",
      "\n",
      "Training Loss: 0.6793\n",
      "0m 3s\n",
      "Validation Loss: 0.8018\n",
      "0m 3s\n",
      "\n",
      "EPOCH 58 of 1000\n",
      "\n",
      "Training Loss: 0.6984\n",
      "0m 3s\n",
      "Validation Loss: 0.8009\n",
      "0m 3s\n",
      "\n",
      "EPOCH 59 of 1000\n",
      "\n",
      "Training Loss: 0.7149\n",
      "0m 3s\n",
      "Validation Loss: 0.7985\n",
      "0m 3s\n",
      "\n",
      "EPOCH 60 of 1000\n",
      "\n",
      "Training Loss: 0.7032\n",
      "0m 3s\n",
      "Validation Loss: 0.7979\n",
      "0m 3s\n",
      "\n",
      "EPOCH 61 of 1000\n",
      "\n",
      "Training Loss: 0.6950\n",
      "0m 3s\n",
      "Validation Loss: 0.8050\n",
      "0m 3s\n",
      "\n",
      "EPOCH 62 of 1000\n",
      "\n",
      "Training Loss: 0.6945\n",
      "0m 3s\n",
      "Validation Loss: 0.7976\n",
      "0m 3s\n",
      "\n",
      "EPOCH 63 of 1000\n",
      "\n",
      "Training Loss: 0.6918\n",
      "0m 3s\n",
      "Validation Loss: 0.7975\n",
      "0m 3s\n",
      "\n",
      "EPOCH 64 of 1000\n",
      "\n",
      "Training Loss: 0.6945\n",
      "0m 3s\n",
      "Validation Loss: 0.7967\n",
      "0m 3s\n",
      "\n",
      "EPOCH 65 of 1000\n",
      "\n",
      "Training Loss: 0.6988\n",
      "0m 3s\n",
      "Validation Loss: 0.7993\n",
      "0m 3s\n",
      "\n",
      "EPOCH 66 of 1000\n",
      "\n",
      "Training Loss: 0.7093\n",
      "0m 3s\n",
      "Validation Loss: 0.7956\n",
      "0m 3s\n",
      "\n",
      "EPOCH 67 of 1000\n",
      "\n",
      "Training Loss: 0.6910\n",
      "0m 3s\n",
      "Validation Loss: 0.7976\n",
      "0m 3s\n",
      "\n",
      "EPOCH 68 of 1000\n",
      "\n",
      "Training Loss: 0.6989\n",
      "0m 3s\n",
      "Validation Loss: 0.8036\n",
      "0m 3s\n",
      "\n",
      "EPOCH 69 of 1000\n",
      "\n",
      "Training Loss: 0.6891\n",
      "0m 3s\n",
      "Validation Loss: 0.7972\n",
      "0m 3s\n",
      "\n",
      "EPOCH 70 of 1000\n",
      "\n",
      "Training Loss: 0.6870\n",
      "0m 3s\n",
      "Validation Loss: 0.7972\n",
      "0m 3s\n",
      "\n",
      "EPOCH 71 of 1000\n",
      "\n",
      "Training Loss: 0.7050\n",
      "0m 3s\n",
      "Validation Loss: 0.7969\n",
      "0m 3s\n",
      "\n",
      "EPOCH 72 of 1000\n",
      "\n",
      "Training Loss: 0.6720\n",
      "0m 3s\n",
      "Validation Loss: 0.7992\n",
      "0m 3s\n",
      "\n",
      "EPOCH 73 of 1000\n",
      "\n",
      "Training Loss: 0.6664\n",
      "0m 3s\n",
      "Validation Loss: 0.7942\n",
      "0m 3s\n",
      "\n",
      "EPOCH 74 of 1000\n",
      "\n",
      "Training Loss: 0.6858\n",
      "0m 3s\n",
      "Validation Loss: 0.8013\n",
      "0m 3s\n",
      "\n",
      "EPOCH 75 of 1000\n",
      "\n",
      "Training Loss: 0.6834\n",
      "0m 3s\n",
      "Validation Loss: 0.7973\n",
      "0m 3s\n",
      "\n",
      "EPOCH 76 of 1000\n",
      "\n",
      "Training Loss: 0.6722\n",
      "0m 3s\n",
      "Validation Loss: 0.7950\n",
      "0m 3s\n",
      "\n",
      "EPOCH 77 of 1000\n",
      "\n",
      "Training Loss: 0.6915\n",
      "0m 3s\n",
      "Validation Loss: 0.7948\n",
      "0m 3s\n",
      "\n",
      "EPOCH 78 of 1000\n",
      "\n",
      "Training Loss: 0.6786\n",
      "0m 3s\n",
      "Validation Loss: 0.8005\n",
      "0m 3s\n",
      "\n",
      "EPOCH 79 of 1000\n",
      "\n",
      "Training Loss: 0.6912\n",
      "0m 3s\n",
      "Validation Loss: 0.7970\n",
      "0m 3s\n",
      "\n",
      "EPOCH 80 of 1000\n",
      "\n",
      "Training Loss: 0.6769\n",
      "0m 3s\n",
      "Validation Loss: 0.7953\n",
      "0m 3s\n",
      "\n",
      "EPOCH 81 of 1000\n",
      "\n",
      "Training Loss: 0.6973\n",
      "0m 3s\n",
      "Validation Loss: 0.7959\n",
      "0m 3s\n",
      "\n",
      "EPOCH 82 of 1000\n",
      "\n",
      "Training Loss: 0.6808\n",
      "0m 3s\n",
      "Validation Loss: 0.7952\n",
      "0m 3s\n",
      "\n",
      "EPOCH 83 of 1000\n",
      "\n",
      "Training Loss: 0.6865\n",
      "0m 3s\n",
      "Validation Loss: 0.7959\n",
      "0m 3s\n",
      "\n",
      "EPOCH 84 of 1000\n",
      "\n",
      "Training Loss: 0.6877\n",
      "0m 3s\n",
      "Validation Loss: 0.7894\n",
      "0m 3s\n",
      "\n",
      "EPOCH 85 of 1000\n",
      "\n",
      "Training Loss: 0.6863\n",
      "0m 3s\n",
      "Validation Loss: 0.7882\n",
      "0m 3s\n",
      "\n",
      "EPOCH 86 of 1000\n",
      "\n",
      "Training Loss: 0.6907\n",
      "0m 3s\n",
      "Validation Loss: 0.7919\n",
      "0m 3s\n",
      "\n",
      "EPOCH 87 of 1000\n",
      "\n",
      "Training Loss: 0.6760\n",
      "0m 3s\n",
      "Validation Loss: 0.7926\n",
      "0m 3s\n",
      "\n",
      "EPOCH 88 of 1000\n",
      "\n",
      "Training Loss: 0.6821\n",
      "0m 3s\n",
      "Validation Loss: 0.7875\n",
      "0m 3s\n",
      "\n",
      "EPOCH 89 of 1000\n",
      "\n",
      "Training Loss: 0.6621\n",
      "0m 3s\n",
      "Validation Loss: 0.7894\n",
      "0m 3s\n",
      "\n",
      "EPOCH 90 of 1000\n",
      "\n",
      "Training Loss: 0.6766\n",
      "0m 3s\n",
      "Validation Loss: 0.7857\n",
      "0m 3s\n",
      "\n",
      "EPOCH 91 of 1000\n",
      "\n",
      "Training Loss: 0.6641\n",
      "0m 3s\n",
      "Validation Loss: 0.7854\n",
      "0m 3s\n",
      "\n",
      "EPOCH 92 of 1000\n",
      "\n",
      "Training Loss: 0.6799\n",
      "0m 3s\n",
      "Validation Loss: 0.7861\n",
      "0m 3s\n",
      "\n",
      "EPOCH 93 of 1000\n",
      "\n",
      "Training Loss: 0.6691\n",
      "0m 3s\n",
      "Validation Loss: 0.7850\n",
      "0m 3s\n",
      "\n",
      "EPOCH 94 of 1000\n",
      "\n",
      "Training Loss: 0.6803\n",
      "0m 3s\n",
      "Validation Loss: 0.7852\n",
      "0m 3s\n",
      "\n",
      "EPOCH 95 of 1000\n",
      "\n",
      "Training Loss: 0.6775\n",
      "0m 3s\n",
      "Validation Loss: 0.7927\n",
      "0m 3s\n",
      "\n",
      "EPOCH 96 of 1000\n",
      "\n",
      "Training Loss: 0.6704\n",
      "0m 3s\n",
      "Validation Loss: 0.7966\n",
      "0m 3s\n",
      "\n",
      "EPOCH 97 of 1000\n",
      "\n",
      "Training Loss: 0.6863\n",
      "0m 3s\n",
      "Validation Loss: 0.7880\n",
      "0m 3s\n",
      "\n",
      "EPOCH 98 of 1000\n",
      "\n",
      "Training Loss: 0.6951\n",
      "0m 3s\n",
      "Validation Loss: 0.7885\n",
      "0m 3s\n",
      "\n",
      "EPOCH 99 of 1000\n",
      "\n",
      "Training Loss: 0.6949\n",
      "0m 3s\n",
      "Validation Loss: 0.7895\n",
      "0m 3s\n",
      "\n",
      "EPOCH 100 of 1000\n",
      "\n",
      "Training Loss: 0.6712\n",
      "0m 3s\n",
      "Validation Loss: 0.7881\n",
      "0m 3s\n",
      "\n",
      "EPOCH 101 of 1000\n",
      "\n",
      "Training Loss: 0.6785\n",
      "0m 3s\n",
      "Validation Loss: 0.7884\n",
      "0m 3s\n",
      "\n",
      "EPOCH 102 of 1000\n",
      "\n",
      "Training Loss: 0.6685\n",
      "0m 3s\n",
      "Validation Loss: 0.7867\n",
      "0m 3s\n",
      "\n",
      "EPOCH 103 of 1000\n",
      "\n",
      "Training Loss: 0.6790\n",
      "0m 3s\n",
      "Validation Loss: 0.7858\n",
      "0m 3s\n",
      "\n",
      "EPOCH 104 of 1000\n",
      "\n",
      "Training Loss: 0.6681\n",
      "0m 3s\n",
      "Validation Loss: 0.7877\n",
      "0m 3s\n",
      "\n",
      "EPOCH 105 of 1000\n",
      "\n",
      "Training Loss: 0.6564\n",
      "0m 3s\n",
      "Validation Loss: 0.7896\n",
      "0m 3s\n",
      "\n",
      "EPOCH 106 of 1000\n",
      "\n",
      "Training Loss: 0.6592\n",
      "0m 3s\n",
      "Validation Loss: 0.7870\n",
      "0m 3s\n",
      "\n",
      "EPOCH 107 of 1000\n",
      "\n",
      "Training Loss: 0.6665\n",
      "0m 3s\n",
      "Validation Loss: 0.7855\n",
      "0m 3s\n",
      "\n",
      "EPOCH 108 of 1000\n",
      "\n",
      "Training Loss: 0.6990\n",
      "0m 3s\n",
      "Validation Loss: 0.7854\n",
      "0m 3s\n",
      "\n",
      "EPOCH 109 of 1000\n",
      "\n",
      "Training Loss: 0.6823\n",
      "0m 3s\n",
      "Validation Loss: 0.7848\n",
      "0m 3s\n",
      "\n",
      "EPOCH 110 of 1000\n",
      "\n",
      "Training Loss: 0.6885\n",
      "0m 3s\n",
      "Validation Loss: 0.7865\n",
      "0m 3s\n",
      "\n",
      "EPOCH 111 of 1000\n",
      "\n",
      "Training Loss: 0.6819\n",
      "0m 3s\n",
      "Validation Loss: 0.7864\n",
      "0m 3s\n",
      "\n",
      "EPOCH 112 of 1000\n",
      "\n",
      "Training Loss: 0.6635\n",
      "0m 3s\n",
      "Validation Loss: 0.7860\n",
      "0m 3s\n",
      "\n",
      "EPOCH 113 of 1000\n",
      "\n",
      "Training Loss: 0.6951\n",
      "0m 3s\n",
      "Validation Loss: 0.7898\n",
      "0m 3s\n",
      "\n",
      "EPOCH 114 of 1000\n",
      "\n",
      "Training Loss: 0.6863\n",
      "0m 3s\n",
      "Validation Loss: 0.7848\n",
      "0m 3s\n",
      "\n",
      "EPOCH 115 of 1000\n",
      "\n",
      "Training Loss: 0.6698\n",
      "0m 3s\n",
      "Validation Loss: 0.7878\n",
      "0m 3s\n",
      "\n",
      "EPOCH 116 of 1000\n",
      "\n",
      "Training Loss: 0.6668\n",
      "0m 3s\n",
      "Validation Loss: 0.7961\n",
      "0m 3s\n",
      "\n",
      "EPOCH 117 of 1000\n",
      "\n",
      "Training Loss: 0.6848\n",
      "0m 3s\n",
      "Validation Loss: 0.7867\n",
      "0m 3s\n",
      "\n",
      "EPOCH 118 of 1000\n",
      "\n",
      "Training Loss: 0.6869\n",
      "0m 3s\n",
      "Validation Loss: 0.7921\n",
      "0m 3s\n",
      "\n",
      "EPOCH 119 of 1000\n",
      "\n",
      "Training Loss: 0.6821\n",
      "0m 3s\n",
      "Validation Loss: 0.7879\n",
      "0m 3s\n",
      "\n",
      "EPOCH 120 of 1000\n",
      "\n",
      "Training Loss: 0.6740\n",
      "0m 3s\n",
      "Validation Loss: 0.7865\n",
      "0m 3s\n",
      "\n",
      "EPOCH 121 of 1000\n",
      "\n",
      "Training Loss: 0.6847\n",
      "0m 3s\n",
      "Validation Loss: 0.7869\n",
      "0m 3s\n",
      "\n",
      "EPOCH 122 of 1000\n",
      "\n",
      "Training Loss: 0.6658\n",
      "0m 3s\n",
      "Validation Loss: 0.7892\n",
      "0m 3s\n",
      "\n",
      "EPOCH 123 of 1000\n",
      "\n",
      "Training Loss: 0.6614\n",
      "0m 3s\n",
      "Validation Loss: 0.7863\n",
      "0m 3s\n",
      "\n",
      "EPOCH 124 of 1000\n",
      "\n",
      "Training Loss: 0.6635\n",
      "0m 3s\n",
      "Validation Loss: 0.7914\n",
      "0m 3s\n",
      "\n",
      "EPOCH 125 of 1000\n",
      "\n",
      "Training Loss: 0.6597\n",
      "0m 3s\n",
      "Validation Loss: 0.7861\n",
      "0m 3s\n",
      "\n",
      "EPOCH 126 of 1000\n",
      "\n",
      "Training Loss: 0.6662\n",
      "0m 3s\n",
      "Validation Loss: 0.7859\n",
      "0m 3s\n",
      "\n",
      "EPOCH 127 of 1000\n",
      "\n",
      "Training Loss: 0.6633\n",
      "0m 3s\n",
      "Validation Loss: 0.7833\n",
      "0m 3s\n",
      "\n",
      "EPOCH 128 of 1000\n",
      "\n",
      "Training Loss: 0.6365\n",
      "0m 3s\n",
      "Validation Loss: 0.5818\n",
      "0m 3s\n",
      "\n",
      "EPOCH 129 of 1000\n",
      "\n",
      "Training Loss: 0.4675\n",
      "0m 3s\n",
      "Validation Loss: 0.5849\n",
      "0m 3s\n",
      "\n",
      "EPOCH 130 of 1000\n",
      "\n",
      "Training Loss: 0.4847\n",
      "0m 3s\n",
      "Validation Loss: 0.5457\n",
      "0m 3s\n",
      "\n",
      "EPOCH 131 of 1000\n",
      "\n",
      "Training Loss: 0.4327\n",
      "0m 3s\n",
      "Validation Loss: 0.5522\n",
      "0m 3s\n",
      "\n",
      "EPOCH 132 of 1000\n",
      "\n",
      "Training Loss: 0.4196\n",
      "0m 3s\n",
      "Validation Loss: 0.5609\n",
      "0m 3s\n",
      "\n",
      "EPOCH 133 of 1000\n",
      "\n",
      "Training Loss: 0.4370\n",
      "0m 3s\n",
      "Validation Loss: 0.5605\n",
      "0m 3s\n",
      "\n",
      "EPOCH 134 of 1000\n",
      "\n",
      "Training Loss: 0.4291\n",
      "0m 3s\n",
      "Validation Loss: 0.5643\n",
      "0m 3s\n",
      "\n",
      "EPOCH 135 of 1000\n",
      "\n",
      "Training Loss: 0.4377\n",
      "0m 3s\n",
      "Validation Loss: 0.5544\n",
      "0m 3s\n",
      "\n",
      "EPOCH 136 of 1000\n",
      "\n",
      "Training Loss: 0.4166\n",
      "0m 3s\n",
      "Validation Loss: 0.5455\n",
      "0m 3s\n",
      "\n",
      "EPOCH 137 of 1000\n",
      "\n",
      "Training Loss: 0.4124\n",
      "0m 3s\n",
      "Validation Loss: 0.5418\n",
      "0m 3s\n",
      "\n",
      "EPOCH 138 of 1000\n",
      "\n",
      "Training Loss: 0.4261\n",
      "0m 3s\n",
      "Validation Loss: 0.5512\n",
      "0m 3s\n",
      "\n",
      "EPOCH 139 of 1000\n",
      "\n",
      "Training Loss: 0.4060\n",
      "0m 3s\n",
      "Validation Loss: 0.5325\n",
      "0m 3s\n",
      "\n",
      "EPOCH 140 of 1000\n",
      "\n",
      "Training Loss: 0.4060\n",
      "0m 3s\n",
      "Validation Loss: 0.5574\n",
      "0m 3s\n",
      "\n",
      "EPOCH 141 of 1000\n",
      "\n",
      "Training Loss: 0.4108\n",
      "0m 3s\n",
      "Validation Loss: 0.5493\n",
      "0m 3s\n",
      "\n",
      "EPOCH 142 of 1000\n",
      "\n",
      "Training Loss: 0.4157\n",
      "0m 3s\n",
      "Validation Loss: 0.5438\n",
      "0m 3s\n",
      "\n",
      "EPOCH 143 of 1000\n",
      "\n",
      "Training Loss: 0.3970\n",
      "0m 3s\n",
      "Validation Loss: 0.5476\n",
      "0m 3s\n",
      "\n",
      "EPOCH 144 of 1000\n",
      "\n",
      "Training Loss: 0.4071\n",
      "0m 3s\n",
      "Validation Loss: 0.5427\n",
      "0m 3s\n",
      "\n",
      "EPOCH 145 of 1000\n",
      "\n",
      "Training Loss: 0.4092\n",
      "0m 3s\n",
      "Validation Loss: 0.5501\n",
      "0m 3s\n",
      "\n",
      "EPOCH 146 of 1000\n",
      "\n",
      "Training Loss: 0.3980\n",
      "0m 3s\n",
      "Validation Loss: 0.5339\n",
      "0m 3s\n",
      "\n",
      "EPOCH 147 of 1000\n",
      "\n",
      "Training Loss: 0.4205\n",
      "0m 3s\n",
      "Validation Loss: 0.5703\n",
      "0m 3s\n",
      "\n",
      "EPOCH 148 of 1000\n",
      "\n",
      "Training Loss: 0.4385\n",
      "0m 3s\n",
      "Validation Loss: 0.5652\n",
      "0m 3s\n",
      "\n",
      "EPOCH 149 of 1000\n",
      "\n",
      "Training Loss: 0.4281\n",
      "0m 3s\n",
      "Validation Loss: 0.5482\n",
      "0m 3s\n",
      "\n",
      "EPOCH 150 of 1000\n",
      "\n",
      "Training Loss: 0.4230\n",
      "0m 3s\n",
      "Validation Loss: 0.5670\n",
      "0m 3s\n",
      "\n",
      "EPOCH 151 of 1000\n",
      "\n",
      "Training Loss: 0.4197\n",
      "0m 3s\n",
      "Validation Loss: 0.5400\n",
      "0m 3s\n",
      "\n",
      "EPOCH 152 of 1000\n",
      "\n",
      "Training Loss: 0.4093\n",
      "0m 3s\n",
      "Validation Loss: 0.5297\n",
      "0m 3s\n",
      "\n",
      "EPOCH 153 of 1000\n",
      "\n",
      "Training Loss: 0.4152\n",
      "0m 3s\n",
      "Validation Loss: 0.5506\n",
      "0m 3s\n",
      "\n",
      "EPOCH 154 of 1000\n",
      "\n",
      "Training Loss: 0.4160\n",
      "0m 3s\n",
      "Validation Loss: 0.5493\n",
      "0m 3s\n",
      "\n",
      "EPOCH 155 of 1000\n",
      "\n",
      "Training Loss: 0.4071\n",
      "0m 3s\n",
      "Validation Loss: 0.5280\n",
      "0m 3s\n",
      "\n",
      "EPOCH 156 of 1000\n",
      "\n",
      "Training Loss: 0.4290\n",
      "0m 3s\n",
      "Validation Loss: 0.5631\n",
      "0m 3s\n",
      "\n",
      "EPOCH 157 of 1000\n",
      "\n",
      "Training Loss: 0.4191\n",
      "0m 3s\n",
      "Validation Loss: 0.5349\n",
      "0m 3s\n",
      "\n",
      "EPOCH 158 of 1000\n",
      "\n",
      "Training Loss: 0.3974\n",
      "0m 3s\n",
      "Validation Loss: 0.5309\n",
      "0m 3s\n",
      "\n",
      "EPOCH 159 of 1000\n",
      "\n",
      "Training Loss: 0.4358\n",
      "0m 3s\n",
      "Validation Loss: 0.5591\n",
      "0m 3s\n",
      "\n",
      "EPOCH 160 of 1000\n",
      "\n",
      "Training Loss: 0.4295\n",
      "0m 3s\n",
      "Validation Loss: 0.5594\n",
      "0m 3s\n",
      "\n",
      "EPOCH 161 of 1000\n",
      "\n",
      "Training Loss: 0.4129\n",
      "0m 3s\n",
      "Validation Loss: 0.5534\n",
      "0m 3s\n",
      "\n",
      "EPOCH 162 of 1000\n",
      "\n",
      "Training Loss: 0.4265\n",
      "0m 3s\n",
      "Validation Loss: 0.5492\n",
      "0m 3s\n",
      "\n",
      "EPOCH 163 of 1000\n",
      "\n",
      "Training Loss: 0.4081\n",
      "0m 3s\n",
      "Validation Loss: 0.5540\n",
      "0m 3s\n",
      "\n",
      "EPOCH 164 of 1000\n",
      "\n",
      "Training Loss: 0.4217\n",
      "0m 3s\n",
      "Validation Loss: 0.5540\n",
      "0m 3s\n",
      "\n",
      "EPOCH 165 of 1000\n",
      "\n",
      "Training Loss: 0.4214\n",
      "0m 3s\n",
      "Validation Loss: 0.5535\n",
      "0m 3s\n",
      "\n",
      "EPOCH 166 of 1000\n",
      "\n",
      "Training Loss: 0.4195\n",
      "0m 3s\n",
      "Validation Loss: 0.5594\n",
      "0m 3s\n",
      "\n",
      "EPOCH 167 of 1000\n",
      "\n",
      "Training Loss: 0.4044\n",
      "0m 3s\n",
      "Validation Loss: 0.5390\n",
      "0m 3s\n",
      "\n",
      "EPOCH 168 of 1000\n",
      "\n",
      "Training Loss: 0.4106\n",
      "0m 3s\n",
      "Validation Loss: 0.5618\n",
      "0m 3s\n",
      "\n",
      "EPOCH 169 of 1000\n",
      "\n",
      "Training Loss: 0.4111\n",
      "0m 3s\n",
      "Validation Loss: 0.5531\n",
      "0m 3s\n",
      "\n",
      "EPOCH 170 of 1000\n",
      "\n",
      "Training Loss: 0.4147\n",
      "0m 3s\n",
      "Validation Loss: 0.5536\n",
      "0m 3s\n",
      "\n",
      "EPOCH 171 of 1000\n",
      "\n",
      "Training Loss: 0.4051\n",
      "0m 3s\n",
      "Validation Loss: 0.5599\n",
      "0m 3s\n",
      "\n",
      "EPOCH 172 of 1000\n",
      "\n",
      "Training Loss: 0.4167\n",
      "0m 3s\n",
      "Validation Loss: 0.5529\n",
      "0m 3s\n",
      "\n",
      "EPOCH 173 of 1000\n",
      "\n",
      "Training Loss: 0.4288\n",
      "0m 3s\n",
      "Validation Loss: 0.5395\n",
      "0m 3s\n",
      "\n",
      "EPOCH 174 of 1000\n",
      "\n",
      "Training Loss: 0.4027\n",
      "0m 3s\n",
      "Validation Loss: 0.5542\n",
      "0m 3s\n",
      "\n",
      "EPOCH 175 of 1000\n",
      "\n",
      "Training Loss: 0.3969\n",
      "0m 3s\n",
      "Validation Loss: 0.5560\n",
      "0m 3s\n",
      "\n",
      "EPOCH 176 of 1000\n",
      "\n",
      "Training Loss: 0.4296\n",
      "0m 3s\n",
      "Validation Loss: 0.5622\n",
      "0m 3s\n",
      "\n",
      "EPOCH 177 of 1000\n",
      "\n",
      "Training Loss: 0.3970\n",
      "0m 3s\n",
      "Validation Loss: 0.5297\n",
      "0m 3s\n",
      "\n",
      "EPOCH 178 of 1000\n",
      "\n",
      "Training Loss: 0.4280\n",
      "0m 3s\n",
      "Validation Loss: 0.5498\n",
      "0m 3s\n",
      "\n",
      "EPOCH 179 of 1000\n",
      "\n",
      "Training Loss: 0.4060\n",
      "0m 3s\n",
      "Validation Loss: 0.5538\n",
      "0m 3s\n",
      "\n",
      "EPOCH 180 of 1000\n",
      "\n",
      "Training Loss: 0.4222\n",
      "0m 3s\n",
      "Validation Loss: 0.5475\n",
      "0m 3s\n",
      "\n",
      "EPOCH 181 of 1000\n",
      "\n",
      "Training Loss: 0.4034\n",
      "0m 3s\n",
      "Validation Loss: 0.5451\n",
      "0m 3s\n",
      "\n",
      "EPOCH 182 of 1000\n",
      "\n",
      "Training Loss: 0.4034\n",
      "0m 3s\n",
      "Validation Loss: 0.5537\n",
      "0m 3s\n",
      "\n",
      "EPOCH 183 of 1000\n",
      "\n",
      "Training Loss: 0.4192\n",
      "0m 3s\n",
      "Validation Loss: 0.5620\n",
      "0m 3s\n",
      "\n",
      "EPOCH 184 of 1000\n",
      "\n",
      "Training Loss: 0.4076\n",
      "0m 3s\n",
      "Validation Loss: 0.5536\n",
      "0m 3s\n",
      "\n",
      "EPOCH 185 of 1000\n",
      "\n",
      "Training Loss: 0.3980\n",
      "0m 3s\n",
      "Validation Loss: 0.5500\n",
      "0m 3s\n",
      "\n",
      "EPOCH 186 of 1000\n",
      "\n",
      "Training Loss: 0.4196\n",
      "0m 3s\n",
      "Validation Loss: 0.5533\n",
      "0m 3s\n",
      "\n",
      "EPOCH 187 of 1000\n",
      "\n",
      "Training Loss: 0.4194\n",
      "0m 3s\n",
      "Validation Loss: 0.5525\n",
      "0m 3s\n",
      "\n",
      "EPOCH 188 of 1000\n",
      "\n",
      "Training Loss: 0.4026\n",
      "0m 3s\n",
      "Validation Loss: 0.5507\n",
      "0m 3s\n",
      "\n",
      "EPOCH 189 of 1000\n",
      "\n",
      "Training Loss: 0.3994\n",
      "0m 3s\n",
      "Validation Loss: 0.5494\n",
      "0m 3s\n",
      "\n",
      "EPOCH 190 of 1000\n",
      "\n",
      "Training Loss: 0.4193\n",
      "0m 3s\n",
      "Validation Loss: 0.5524\n",
      "0m 3s\n",
      "\n",
      "EPOCH 191 of 1000\n",
      "\n",
      "Training Loss: 0.3878\n",
      "0m 3s\n",
      "Validation Loss: 0.5534\n",
      "0m 3s\n",
      "\n",
      "EPOCH 192 of 1000\n",
      "\n",
      "Training Loss: 0.4163\n",
      "0m 3s\n",
      "Validation Loss: 0.5519\n",
      "0m 3s\n",
      "\n",
      "EPOCH 193 of 1000\n",
      "\n",
      "Training Loss: 0.3989\n",
      "0m 3s\n",
      "Validation Loss: 0.5476\n",
      "0m 3s\n",
      "\n",
      "EPOCH 194 of 1000\n",
      "\n",
      "Training Loss: 0.4019\n",
      "0m 3s\n",
      "Validation Loss: 0.5580\n",
      "0m 3s\n",
      "\n",
      "EPOCH 195 of 1000\n",
      "\n",
      "Training Loss: 0.4014\n",
      "0m 3s\n",
      "Validation Loss: 0.5572\n",
      "0m 3s\n",
      "\n",
      "EPOCH 196 of 1000\n",
      "\n",
      "Training Loss: 0.4043\n",
      "0m 3s\n",
      "Validation Loss: 0.5314\n",
      "0m 3s\n",
      "\n",
      "EPOCH 197 of 1000\n",
      "\n",
      "Training Loss: 0.3860\n",
      "0m 3s\n",
      "Validation Loss: 0.5496\n",
      "0m 3s\n",
      "\n",
      "EPOCH 198 of 1000\n",
      "\n",
      "Training Loss: 0.4067\n",
      "0m 3s\n",
      "Validation Loss: 0.5565\n",
      "0m 3s\n",
      "\n",
      "EPOCH 199 of 1000\n",
      "\n",
      "Training Loss: 0.4206\n",
      "0m 3s\n",
      "Validation Loss: 0.5506\n",
      "0m 3s\n",
      "\n",
      "EPOCH 200 of 1000\n",
      "\n",
      "Training Loss: 0.4151\n",
      "0m 3s\n",
      "Validation Loss: 0.5554\n",
      "0m 3s\n",
      "\n",
      "EPOCH 201 of 1000\n",
      "\n",
      "Training Loss: 0.3960\n",
      "0m 3s\n",
      "Validation Loss: 0.5474\n",
      "0m 3s\n",
      "\n",
      "EPOCH 202 of 1000\n",
      "\n",
      "Training Loss: 0.4066\n",
      "0m 3s\n",
      "Validation Loss: 0.5484\n",
      "0m 3s\n",
      "\n",
      "EPOCH 203 of 1000\n",
      "\n",
      "Training Loss: 0.4019\n",
      "0m 3s\n",
      "Validation Loss: 0.5475\n",
      "0m 3s\n",
      "\n",
      "EPOCH 204 of 1000\n",
      "\n",
      "Training Loss: 0.3954\n",
      "0m 3s\n",
      "Validation Loss: 0.5428\n",
      "0m 3s\n",
      "\n",
      "EPOCH 205 of 1000\n",
      "\n",
      "Training Loss: 0.4066\n",
      "0m 3s\n",
      "Validation Loss: 0.5539\n",
      "0m 3s\n",
      "\n",
      "EPOCH 206 of 1000\n",
      "\n",
      "Training Loss: 0.4174\n",
      "0m 3s\n",
      "Validation Loss: 0.5536\n",
      "0m 3s\n",
      "\n",
      "EPOCH 207 of 1000\n",
      "\n",
      "Training Loss: 0.3824\n",
      "0m 3s\n",
      "Validation Loss: 0.5363\n",
      "0m 3s\n",
      "\n",
      "EPOCH 208 of 1000\n",
      "\n",
      "Training Loss: 0.4033\n",
      "0m 3s\n",
      "Validation Loss: 0.5498\n",
      "0m 3s\n",
      "\n",
      "EPOCH 209 of 1000\n",
      "\n",
      "Training Loss: 0.3915\n",
      "0m 3s\n",
      "Validation Loss: 0.5482\n",
      "0m 3s\n",
      "\n",
      "EPOCH 210 of 1000\n",
      "\n",
      "Training Loss: 0.3954\n",
      "0m 3s\n",
      "Validation Loss: 0.5448\n",
      "0m 3s\n",
      "\n",
      "EPOCH 211 of 1000\n",
      "\n",
      "Training Loss: 0.3855\n",
      "0m 3s\n",
      "Validation Loss: 0.5367\n",
      "0m 3s\n",
      "\n",
      "EPOCH 212 of 1000\n",
      "\n",
      "Training Loss: 0.4192\n",
      "0m 3s\n",
      "Validation Loss: 0.5561\n",
      "0m 3s\n",
      "\n",
      "EPOCH 213 of 1000\n",
      "\n",
      "Training Loss: 0.4049\n",
      "0m 3s\n",
      "Validation Loss: 0.5523\n",
      "0m 3s\n",
      "\n",
      "EPOCH 214 of 1000\n",
      "\n",
      "Training Loss: 0.4145\n",
      "0m 3s\n",
      "Validation Loss: 0.5490\n",
      "0m 3s\n",
      "\n",
      "EPOCH 215 of 1000\n",
      "\n",
      "Training Loss: 0.4114\n",
      "0m 3s\n",
      "Validation Loss: 0.5507\n",
      "0m 3s\n",
      "\n",
      "EPOCH 216 of 1000\n",
      "\n",
      "Training Loss: 0.3913\n",
      "0m 3s\n",
      "Validation Loss: 0.5389\n",
      "0m 3s\n",
      "\n",
      "EPOCH 217 of 1000\n",
      "\n",
      "Training Loss: 0.4016\n",
      "0m 3s\n",
      "Validation Loss: 0.5373\n",
      "0m 3s\n",
      "\n",
      "EPOCH 218 of 1000\n",
      "\n",
      "Training Loss: 0.4136\n",
      "0m 3s\n",
      "Validation Loss: 0.5526\n",
      "0m 3s\n",
      "\n",
      "EPOCH 219 of 1000\n",
      "\n",
      "Training Loss: 0.3986\n",
      "0m 3s\n",
      "Validation Loss: 0.5438\n",
      "0m 3s\n",
      "\n",
      "EPOCH 220 of 1000\n",
      "\n",
      "Training Loss: 0.4010\n",
      "0m 3s\n",
      "Validation Loss: 0.5380\n",
      "0m 3s\n",
      "\n",
      "EPOCH 221 of 1000\n",
      "\n",
      "Training Loss: 0.3960\n",
      "0m 3s\n",
      "Validation Loss: 0.5514\n",
      "0m 3s\n",
      "\n",
      "EPOCH 222 of 1000\n",
      "\n",
      "Training Loss: 0.3965\n",
      "0m 3s\n",
      "Validation Loss: 0.5449\n",
      "0m 3s\n",
      "\n",
      "EPOCH 223 of 1000\n",
      "\n",
      "Training Loss: 0.4058\n",
      "0m 3s\n",
      "Validation Loss: 0.5341\n",
      "0m 3s\n",
      "\n",
      "EPOCH 224 of 1000\n",
      "\n",
      "Training Loss: 0.4372\n",
      "0m 3s\n",
      "Validation Loss: 0.5790\n",
      "0m 3s\n",
      "\n",
      "EPOCH 225 of 1000\n",
      "\n",
      "Training Loss: 0.4486\n",
      "0m 3s\n",
      "Validation Loss: 0.5666\n",
      "0m 3s\n",
      "\n",
      "EPOCH 226 of 1000\n",
      "\n",
      "Training Loss: 0.4296\n",
      "0m 3s\n",
      "Validation Loss: 0.5521\n",
      "0m 3s\n",
      "\n",
      "EPOCH 227 of 1000\n",
      "\n",
      "Training Loss: 0.4095\n",
      "0m 3s\n",
      "Validation Loss: 0.5497\n",
      "0m 3s\n",
      "\n",
      "EPOCH 228 of 1000\n",
      "\n",
      "Training Loss: 0.4134\n",
      "0m 3s\n",
      "Validation Loss: 0.5275\n",
      "0m 3s\n",
      "\n",
      "EPOCH 229 of 1000\n",
      "\n",
      "Training Loss: 0.3958\n",
      "0m 3s\n",
      "Validation Loss: 0.5581\n",
      "0m 3s\n",
      "\n",
      "EPOCH 230 of 1000\n",
      "\n",
      "Training Loss: 0.4047\n",
      "0m 3s\n",
      "Validation Loss: 0.5585\n",
      "0m 3s\n",
      "\n",
      "EPOCH 231 of 1000\n",
      "\n",
      "Training Loss: 0.4068\n",
      "0m 3s\n",
      "Validation Loss: 0.5456\n",
      "0m 3s\n",
      "\n",
      "EPOCH 232 of 1000\n",
      "\n",
      "Training Loss: 0.4271\n",
      "0m 3s\n",
      "Validation Loss: 0.5601\n",
      "0m 3s\n",
      "\n",
      "EPOCH 233 of 1000\n",
      "\n",
      "Training Loss: 0.4149\n",
      "0m 3s\n",
      "Validation Loss: 0.5534\n",
      "0m 3s\n",
      "\n",
      "EPOCH 234 of 1000\n",
      "\n",
      "Training Loss: 0.4122\n",
      "0m 3s\n",
      "Validation Loss: 0.5527\n",
      "0m 3s\n",
      "\n",
      "EPOCH 235 of 1000\n",
      "\n",
      "Training Loss: 0.4231\n",
      "0m 3s\n",
      "Validation Loss: 0.5529\n",
      "0m 3s\n",
      "\n",
      "EPOCH 236 of 1000\n",
      "\n",
      "Training Loss: 0.3978\n",
      "0m 3s\n",
      "Validation Loss: 0.5464\n",
      "0m 3s\n",
      "\n",
      "EPOCH 237 of 1000\n",
      "\n",
      "Training Loss: 0.3997\n",
      "0m 3s\n",
      "Validation Loss: 0.5543\n",
      "0m 3s\n",
      "\n",
      "EPOCH 238 of 1000\n",
      "\n",
      "Training Loss: 0.4021\n",
      "0m 3s\n",
      "Validation Loss: 0.5535\n",
      "0m 3s\n",
      "\n",
      "EPOCH 239 of 1000\n",
      "\n",
      "Training Loss: 0.4095\n",
      "0m 3s\n",
      "Validation Loss: 0.5443\n",
      "0m 3s\n",
      "\n",
      "EPOCH 240 of 1000\n",
      "\n",
      "Training Loss: 0.3903\n",
      "0m 3s\n",
      "Validation Loss: 0.5413\n",
      "0m 3s\n",
      "\n",
      "EPOCH 241 of 1000\n",
      "\n",
      "Training Loss: 0.4023\n",
      "0m 3s\n",
      "Validation Loss: 0.5529\n",
      "0m 3s\n",
      "\n",
      "EPOCH 242 of 1000\n",
      "\n",
      "Training Loss: 0.4032\n",
      "0m 3s\n",
      "Validation Loss: 0.5478\n",
      "0m 3s\n",
      "\n",
      "EPOCH 243 of 1000\n",
      "\n",
      "Training Loss: 0.3817\n",
      "0m 3s\n",
      "Validation Loss: 0.5347\n",
      "0m 3s\n",
      "\n",
      "EPOCH 244 of 1000\n",
      "\n",
      "Training Loss: 0.4172\n",
      "0m 3s\n",
      "Validation Loss: 0.5556\n",
      "0m 3s\n",
      "\n",
      "EPOCH 245 of 1000\n",
      "\n",
      "Training Loss: 0.3900\n",
      "0m 3s\n",
      "Validation Loss: 0.5507\n",
      "0m 3s\n",
      "\n",
      "EPOCH 246 of 1000\n",
      "\n",
      "Training Loss: 0.4039\n",
      "0m 3s\n",
      "Validation Loss: 0.5434\n",
      "0m 3s\n",
      "\n",
      "EPOCH 247 of 1000\n",
      "\n",
      "Training Loss: 0.4216\n",
      "0m 3s\n",
      "Validation Loss: 0.5561\n",
      "0m 3s\n",
      "\n",
      "EPOCH 248 of 1000\n",
      "\n",
      "Training Loss: 0.4158\n",
      "0m 3s\n",
      "Validation Loss: 0.5473\n",
      "0m 3s\n",
      "\n",
      "EPOCH 249 of 1000\n",
      "\n",
      "Training Loss: 0.3927\n",
      "0m 3s\n",
      "Validation Loss: 0.5054\n",
      "0m 3s\n",
      "\n",
      "EPOCH 250 of 1000\n",
      "\n",
      "Training Loss: 0.3986\n",
      "0m 3s\n",
      "Validation Loss: 0.5564\n",
      "0m 3s\n",
      "\n",
      "EPOCH 251 of 1000\n",
      "\n",
      "Training Loss: 0.4112\n",
      "0m 3s\n",
      "Validation Loss: 0.5555\n",
      "0m 3s\n",
      "\n",
      "EPOCH 252 of 1000\n",
      "\n",
      "Training Loss: 0.3898\n",
      "0m 3s\n",
      "Validation Loss: 0.5566\n",
      "0m 3s\n",
      "\n",
      "EPOCH 253 of 1000\n",
      "\n",
      "Training Loss: 0.4061\n",
      "0m 3s\n",
      "Validation Loss: 0.5571\n",
      "0m 3s\n",
      "\n",
      "EPOCH 254 of 1000\n",
      "\n",
      "Training Loss: 0.3952\n",
      "0m 3s\n",
      "Validation Loss: 0.5509\n",
      "0m 3s\n",
      "\n",
      "EPOCH 255 of 1000\n",
      "\n",
      "Training Loss: 0.3826\n",
      "0m 3s\n",
      "Validation Loss: 0.5447\n",
      "0m 3s\n",
      "\n",
      "EPOCH 256 of 1000\n",
      "\n",
      "Training Loss: 0.4003\n",
      "0m 3s\n",
      "Validation Loss: 0.5507\n",
      "0m 3s\n",
      "\n",
      "EPOCH 257 of 1000\n",
      "\n",
      "Training Loss: 0.4182\n",
      "0m 3s\n",
      "Validation Loss: 0.5434\n",
      "0m 3s\n",
      "\n",
      "EPOCH 258 of 1000\n",
      "\n",
      "Training Loss: 0.3837\n",
      "0m 3s\n",
      "Validation Loss: 0.5291\n",
      "0m 3s\n",
      "\n",
      "EPOCH 259 of 1000\n",
      "\n",
      "Training Loss: 0.4062\n",
      "0m 3s\n",
      "Validation Loss: 0.5576\n",
      "0m 3s\n",
      "\n",
      "EPOCH 260 of 1000\n",
      "\n",
      "Training Loss: 0.4095\n",
      "0m 3s\n",
      "Validation Loss: 0.5506\n",
      "0m 3s\n",
      "\n",
      "EPOCH 261 of 1000\n",
      "\n",
      "Training Loss: 0.3931\n",
      "0m 3s\n",
      "Validation Loss: 0.5456\n",
      "0m 3s\n",
      "\n",
      "EPOCH 262 of 1000\n",
      "\n",
      "Training Loss: 0.4159\n",
      "0m 3s\n",
      "Validation Loss: 0.5512\n",
      "0m 3s\n",
      "\n",
      "EPOCH 263 of 1000\n",
      "\n",
      "Training Loss: 0.4127\n",
      "0m 3s\n",
      "Validation Loss: 0.5500\n",
      "0m 3s\n",
      "\n",
      "EPOCH 264 of 1000\n",
      "\n",
      "Training Loss: 0.4098\n",
      "0m 3s\n",
      "Validation Loss: 0.5458\n",
      "0m 3s\n",
      "\n",
      "EPOCH 265 of 1000\n",
      "\n",
      "Training Loss: 0.3946\n",
      "0m 3s\n",
      "Validation Loss: 0.5439\n",
      "0m 3s\n",
      "\n",
      "EPOCH 266 of 1000\n",
      "\n",
      "Training Loss: 0.3947\n",
      "0m 3s\n",
      "Validation Loss: 0.5509\n",
      "0m 3s\n",
      "\n",
      "EPOCH 267 of 1000\n",
      "\n",
      "Training Loss: 0.3895\n",
      "0m 3s\n",
      "Validation Loss: 0.5515\n",
      "0m 3s\n",
      "\n",
      "EPOCH 268 of 1000\n",
      "\n",
      "Training Loss: 0.3908\n",
      "0m 3s\n",
      "Validation Loss: 0.5456\n",
      "0m 3s\n",
      "\n",
      "EPOCH 269 of 1000\n",
      "\n",
      "Training Loss: 0.3939\n",
      "0m 3s\n",
      "Validation Loss: 0.5524\n",
      "0m 3s\n",
      "\n",
      "EPOCH 270 of 1000\n",
      "\n",
      "Training Loss: 0.3826\n",
      "0m 3s\n",
      "Validation Loss: 0.5537\n",
      "0m 3s\n",
      "\n",
      "EPOCH 271 of 1000\n",
      "\n",
      "Training Loss: 0.3939\n",
      "0m 3s\n",
      "Validation Loss: 0.5480\n",
      "0m 3s\n",
      "\n",
      "EPOCH 272 of 1000\n",
      "\n",
      "Training Loss: 0.3907\n",
      "0m 3s\n",
      "Validation Loss: 0.5438\n",
      "0m 3s\n",
      "\n",
      "EPOCH 273 of 1000\n",
      "\n",
      "Training Loss: 0.4072\n",
      "0m 3s\n",
      "Validation Loss: 0.5526\n",
      "0m 3s\n",
      "\n",
      "EPOCH 274 of 1000\n",
      "\n",
      "Training Loss: 0.3855\n",
      "0m 3s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-cbf290c0899d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdice_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = unet();\n",
    "\n",
    "if torch.cuda.is_available(): #use gpu if available\n",
    "    model.cuda()\n",
    "\n",
    "import time\n",
    "import copy\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "print(\"Training Started!\")\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "converge_epoch = []\n",
    "for epoch in range(num_epochs):\n",
    "    ########################### Training #####################################\n",
    "    print(\"\\nEPOCH \" +str(epoch+1)+\" of \"+str(num_epochs)+\"\\n\")\n",
    "    # Please design your own training section\n",
    "    since = time.time()\n",
    "    losses=[]\n",
    "    for _, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.float())\n",
    "        labels = Variable(labels.float())\n",
    "\n",
    "        ### Data Augmentation\n",
    "        i_ver = copy.deepcopy(images)  ## image vertical flip\n",
    "        i_hor = copy.deepcopy(images)  ## image horizontal flip\n",
    "        v_ver = copy.deepcopy(labels)  ## labels vertical flip\n",
    "        v_hor = copy.deepcopy(labels)  ## labels horizontal flip\n",
    "\n",
    "        for i in range(train_batch_size):\n",
    "            for p in range(3):\n",
    "                i_ver[i][p] = torch.from_numpy(np.flipud(images[i][p]).copy())\n",
    "                i_hor[i][p] = torch.from_numpy(np.fliplr(images[i][p]).copy())\n",
    "            for q in range(8):\n",
    "                v_ver[i][q] = torch.from_numpy(np.flipud(labels[i][q]).copy())\n",
    "                v_hor[i][q] = torch.from_numpy(np.fliplr(labels[i][q]).copy())\n",
    "        \n",
    "        # Regular Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images.cuda())\n",
    "        loss = dice_loss(outputs, labels.cuda())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.data.item())\n",
    "        \n",
    "        # Vertical Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(i_ver.cuda())\n",
    "        loss = dice_loss(outputs, v_ver.cuda())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.data.item())\n",
    "        \n",
    "        # Horizontal Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(i_hor.cuda())\n",
    "        loss = dice_loss(outputs, v_hor.cuda())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.data.item())\n",
    "\n",
    "        break\n",
    "    ### Average Batch Loss\n",
    "    mean_loss = sum(losses)/len(losses)\n",
    "    print(\"Training Loss: %.4f\" % (mean_loss))\n",
    "    train_losses.append(mean_loss)\n",
    "    converge_epoch.append(epoch)\n",
    "    \n",
    "    ### Timing\n",
    "    time_elapsed = time.time() - since\n",
    "    print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    ########################### Validation #####################################\n",
    "    # Please design your own validation section\n",
    "    model.eval()\n",
    "    since = time.time()\n",
    "    losses=[]\n",
    "\n",
    "    for (images, labels) in validation_loader:\n",
    "        images = Variable(images.float())\n",
    "        labels = Variable(labels.float())\n",
    "\n",
    "        outputs = model(images.cuda())\n",
    "        loss = dice_loss(outputs, labels.cuda())\n",
    "        losses.append(loss.data.item())\n",
    "    \n",
    "    ### Average Batch Loss\n",
    "    mean_loss = sum(losses)/len(losses)\n",
    "    print(\"Validation Loss: %.4f\" % (mean_loss))\n",
    "    val_losses.append(mean_loss)\n",
    "    \n",
    "    ### Timing\n",
    "    time_elapsed = time.time() - since\n",
    "    print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    ### Early Stopping\n",
    "    if epoch > 40 and abs(mean_loss - val_losses[epoch-1]) <= 1e-6:\n",
    "        print('The validation loss converges')\n",
    "        break\n",
    "plt.plot(converge_epoch, train_losses, label = \"training loss\")\n",
    "plt.plot(converge_epoch, val_losses, label = \"validation loss\")\n",
    "plt.xlabel('epoches')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 954,
     "status": "ok",
     "timestamp": 1555126267114,
     "user": {
      "displayName": "Xi Wang",
      "photoUrl": "",
      "userId": "14282931480726339478"
     },
     "user_tz": 240
    },
    "id": "gfRS-jMeO7rK",
    "outputId": "30c7c56a-f39f-40d9-f2be-1aa4d413e2e8"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxUFFQVEURXFQUVAURVEc/h/t69ejVKeT9AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(converge_epoch[:-1], train_losses[:-1], label = \"training loss\")\n",
    "plt.plot(converge_epoch[:-1], val_losses, label = \"validation loss\")\n",
    "plt.xlabel('epoches')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3708,
     "status": "ok",
     "timestamp": 1555126277902,
     "user": {
      "displayName": "Xi Wang",
      "photoUrl": "",
      "userId": "14282931480726339478"
     },
     "user_tz": 240
    },
    "id": "yusfwz_ib5DF",
    "outputId": "fe4ac734-4bc4-4fef-ef06-61f2c7347fc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing performance - DICE score: 0.4954\n",
      "0m 3s\n"
     ]
    }
   ],
   "source": [
    "########################### Testing #####################################\n",
    "# Please design your own validation section\n",
    "model.eval()\n",
    "since = time.time()\n",
    "losses=[]\n",
    "\n",
    "for (images, labels) in test_loader:\n",
    "    images = Variable(images.float())\n",
    "    labels = Variable(labels.float())\n",
    "\n",
    "    outputs = model(images.cuda())\n",
    "    loss = dice_loss(outputs, labels.cuda())\n",
    "    losses.append(loss.data.item())\n",
    "\n",
    "### Average Batch Loss\n",
    "mean_loss = sum(losses)/len(losses)\n",
    "print(\"Testing performance - DICE score: %.4f\" % (1-mean_loss))\n",
    "\n",
    "### Timing\n",
    "time_elapsed = time.time() - since\n",
    "print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GgqA56p3vBoT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Xi_Wang_HW6_Q1cHe(flip).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
