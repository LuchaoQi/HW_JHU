{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 635,
     "status": "ok",
     "timestamp": 1552690956574,
     "user": {
      "displayName": "Xi Wang",
      "photoUrl": "",
      "userId": "14282931480726339478"
     },
     "user_tz": 240
    },
    "id": "x5utOTxul6VA",
    "outputId": "4344d75c-d5a7-417c-f9f5-5437536e681e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/mydrive; to attempt to forcibly remount, call drive.mount(\"/content/mydrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eEEL8kKkl7Qg"
   },
   "outputs": [],
   "source": [
    "# This code is used for JHU CS 482/682: Deep Learning 2019 Spring Homework 4\n",
    "# Copyright @ Johns Hopkins University, Cong Gao, cgao11@jhu.edu\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "# IMPORTANT: This function is used for de-normalizing image to original domain,\n",
    "# please use this if you want to visualize/recover your output result\n",
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 1, 28, 28)\n",
    "    return x\n",
    "\n",
    "# These hyperparameters can be changed\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# This is a recommended normalization method. You can design your own\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, ), (0.5, ))\n",
    "])\n",
    "\n",
    "# Dataset wiill be downloaded to './data' folder\n",
    "dataset = FashionMNIST('./data', transform=img_transform, download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# This is our provided network autoencoder class, you are encouraged to change\n",
    "# the structure settings\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=3, padding=1),  # b, 16, 10, 10\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),  # b, 16, 5, 5\n",
    "            nn.Conv2d(16, 8, 3, stride=2, padding=1),  # b, 8, 3, 3\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=1)  # b, 8, 2, 2\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 16, 3, stride=2),  # b, 16, 5, 5\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 8, 5, stride=3, padding=1),  # b, 8, 15, 15\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(8, 1, 2, stride=2, padding=1),  # b, 1, 28, 28\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        # make sure you keep this layer during your autoencoder training, this\n",
    "        # will be used for Q3-(c) fully connected layer\n",
    "        self.linear = nn.Linear(32,10)\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        # IMPORTANT: in Q3-(c), please delete the above decoder layer, and use\n",
    "        # the linear layer to build fully-connection layers.\n",
    "        return x\n",
    "\n",
    "\n",
    "model = autoencoder()\n",
    "if torch.cuda.is_available(): #use gpu if available\n",
    "    model.cuda()\n",
    "\n",
    "# In Q3-(c), you need to load your model using the following command\n",
    "#model.load_state_dict(torch.load('./yourmodel.pth'))\n",
    "\n",
    "# Setting requires_grad to False will 'freeze' the parameters during training\n",
    "#model.encoder.requires_grad = False\n",
    "\n",
    "# This is a recommended optimizer setting\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vrgA3oAOwGhy"
   },
   "outputs": [],
   "source": [
    "\n",
    "# criterion = # Please figure this out for loss function\n",
    "\n",
    "# Training & Validation\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        # =================== show your work =====================\n",
    "\n",
    "\n",
    "# This is used to save your model, please remember to include it\n",
    "torch.save(model.state_dict(), './yourmodel.pth')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Xi_Wang_HW4_Q2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
